 @article{CziborJimenez-GomezList2019, title={The Dozen Things Experimental Economists Should Do (More of)}, volume={86}, rights={© 2019 by the Southern Economic Association}, ISSN={2325-8012}, DOI={10.1002/soej.12392}, abstractNote={What was once broadly viewed as an impossibility—learning from experimental data in economics—has now become commonplace. Governmental bodies, think tanks, and corporations around the world employ teams of experimental researchers to answer their most pressing questions. For their part, in the past two decades academics have begun to more actively partner with organizations to generate data via field experimentation. Although this revolution in evidence-based approaches has served to deepen the economic science, recently a credibility crisis has caused even the most ardent experimental proponents to pause. This study takes a step back from the burgeoning experimental literature and introduces 12 actions that might help to alleviate this credibility crisis and raise experimental economics to an even higher level. In this way, we view our “12 action wish list” as discussion points to enrich the field.}, number={2}, journal={Southern Economic Journal}, author={Czibor, Eszter and Jimenez-Gomez, David and List, John A.}, year={2019}, pages={371–432}, language={en} }
 
@article{BurligPreonasWoerman2020, title={Panel data and experimental design}, volume={144}, ISSN={0304-3878}, DOI={10.1016/j.jdeveco.2020.102458}, abstractNote={How should researchers design panel data experiments? We analytically derive the variance of panel estimators, informing power calculations in panel data settings. We generalize Frison and Pocock (1992) to fully arbitrary error structures, thereby extending McKenzie (2012) to allow for non-constant serial correlation. Using Monte Carlo simulations and real-world panel data, we demonstrate that failing to account for arbitrary serial correlation ex ante yields experiments that are incorrectly powered under proper inference. By contrast, our “serial-correlation-robust” power calculations achieve correctly powered experiments in both simulated and real data. We discuss the implications of these results, and introduce a new software package to facilitate proper power calculations in practice.}, journal={Journal of Development Economics}, author={Burlig, Fiona and Preonas, Louis and Woerman, Matt}, year={2020}, month={May}, pages={102458}, language={en} }

 @article{McKenzie2012, title={Beyond baseline and follow-up: The case for more T in experiments}, volume={99}, ISSN={0304-3878}, DOI={10.1016/j.jdeveco.2012.01.002}, abstractNote={The vast majority of randomized experiments in economics rely on a single baseline and single follow-up survey. While such a design is suitable for study of highly autocorrelated and relatively precisely measured outcomes in the health and education domains, it is unlikely to be optimal for measuring noisy and relatively less autocorrelated outcomes such as business profits, and household incomes and expenditures. Taking multiple measurements of such outcomes at relatively short intervals allows one to average out noise, increasing power. When the outcomes have low autocorrelation and budget is limited, it can make sense to do no baseline at all. Moreover, I show how for such outcomes, more power can be achieved with multiple follow-ups than allocating the same total sample size over a single follow-up and baseline. I also highlight the large gains in power from ANCOVA analysis rather than difference-in-differences analysis when autocorrelations are low.}, number={2}, journal={Journal of Development Economics}, author={McKenzie, David}, year={2012}, month={Nov}, pages={210–221}, language={en} }

 @inbook{AtheyImbens2017, series={Handbook of Field Experiments}, title={The Econometrics of Randomized Experiments}, volume={1}, url={https://www.sciencedirect.com/science/article/pii/S2214658X16300174}, DOI={10.1016/bs.hefe.2016.10.003}, abstractNote={In this chapter, we present econometric and statistical methods for analyzing randomized experiments. For basic experiments, we stress randomization-based inference as opposed to sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. We show how this perspective relates to regression analyses for randomized experiments. We discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. We also discuss complications in randomized experiments such as noncompliance. In the presence of noncompliance, we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider, in detail, estimation and inference for heterogenous treatment effects in settings with (possibly many) covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.}, booktitle={Handbook of Economic Field Experiments}, publisher={North-Holland}, author={Athey, S. and Imbens, G. W.}, editor={Banerjee, Abhijit Vinayak and Duflo, Esther}, year={2017}, month={Jan}, pages={73–140}, collection={Handbook of Field Experiments}, language={en} }


 @inbook{DufloGlennersterKremer2007, title={Using Randomization in Development Economics Research: A Toolkit}, volume={4}, url={https://www.sciencedirect.com/science/article/pii/S1573447107040612}, DOI={10.1016/S1573-4471(07)04061-2}, abstractNote={This paper is a practical guide (a toolkit) for researchers, students and practitioners wishing to introduce randomization as part of a research design in the field. It first covers the rationale for the use of randomization, as a solution to selection bias and a partial solution to publication biases. Second, it discusses various ways in which randomization can be practically introduced in a field settings. Third, it discusses designs issues such as sample size requirements, stratification, level of randomization and data collection methods. Fourth, it discusses how to analyze data from randomized evaluations when there are departures from the basic framework. It reviews in particular how to handle imperfect compliance and externalities. Finally, it discusses some of the issues involved in drawing general conclusions from randomized evaluations, including the necessary use of theory as a guide when designing evaluations and interpreting results.}, booktitle={Handbook of Development Economics}, publisher={Elsevier}, author={Duflo, Esther and Glennerster, Rachel and Kremer, Michael}, editor={Schultz, T. Paul and Strauss, John A.}, year={2007}, month={Jan}, pages={3895–3962}, language={en} }


 @article{GelmanCarlin2014, title={Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors}, volume={9}, ISSN={1745-6916}, DOI={10.1177/1745691614551642}, abstractNote={Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.}, number={6}, journal={Perspectives on Psychological Science}, publisher={SAGE Publications Inc}, author={Gelman, Andrew and Carlin, John}, year={2014}, month={Nov}, pages={641–651}, language={en} }


 @book{DonnerKlar2010, address={London}, edition={1st edition}, title={Design and Analysis of Cluster Randomization Trials in Health Research}, ISBN={978-0-470-71100-2}, publisher={Wiley}, author={Donner, Allan and Klar, Neil}, year={2010}, month={May}, language={English} }

 @book{McConnellVera-Hernandez2015, title={Going beyond simple sample size calculations: a practitioner’s guide}, url={https://www.ifs.org.uk/uploads/publications/wps/WP201517_update_Sep15.pdf}, DOI={10.1920/wp.ifs.2015.1517}, abstractNote={Basic methods to compute required sample sizes are well understood and supported by widely available software. However, the sophistication of the methods commonly used has not kept pace with the complexity of commonly employed experimental designs. We compile available methods for sample size calculations for continuous and binary outcomes with and without covariates, for both clustered and non-clustered RCTs. Formulae for both panel data and unbalanced designs are provided. Extensions include methods to: (1) optimise the sample when costs constraints are binding, (2) compute the power of a complex design by simulation, and (3) adjust calculations for multiple testing.}, institution={Institute for Fiscal Studies}, author={McConnell, Brendon and Vera-Hernandez, Marcos}, year={2015}, month={Sep}, language={en} }

 @book{GelmanHillVehtari2020, address={Cambridge}, series={Analytical Methods for Social Research}, title={Regression and Other Stories}, ISBN={978-1-107-02398-7}, url={https://www.cambridge.org/core/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C}, DOI={10.1017/9781139161879}, abstractNote={Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors’ experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.}, publisher={Cambridge University Press}, author={Gelman, Andrew and Hill, Jennifer and Vehtari, Aki}, year={2020}, collection={Analytical Methods for Social Research} }


 @book{Huntington-Klein2022, address={Boca Raton}, edition={1st edition}, title={The Effect}, ISBN={978-1-03-212578-7}, abstractNote={The Effect: An Introduction to Research Design and Causality is about research design, specifically concerning research that uses observational data to make a causal inference. It is separated into two halves, each with different approaches to that subject. The first half goes through the concepts of causality, with very little in the way of estimation. It introduces the concept of identification thoroughly and clearly and discusses it as a process of trying to isolate variation that has a causal interpretation. Subjects include heavy emphasis on data-generating processes and causal diagrams.Concepts are demonstrated with a heavy emphasis on graphical intuition and the question of what we do to data. When we “add a control variable” what does that actually do?Key Features: • Extensive code examples in R, Stata, and Python• Chapters on overlooked topics in econometrics classes: heterogeneous treatment effects, simulation and power analysis, new cutting-edge methods, and uncomfortable ignored assumptions• An easy-to-read conversational tone• Up-to-date coverage of methods with fast-moving literatures like difference-in-differences}, publisher={Routledge}, author={Huntington-Klein, Nick}, year={2022}, month={Jan}, language={English} }

 @article{LevittList2009, title={Field experiments in economics: The past, the present, and the future}, volume={53}, ISSN={0014-2921}, DOI={10.1016/j.euroecorev.2008.12.001}, abstractNote={This study presents an overview of modern field experiments and their usage in economics. Our discussion focuses on three distinct periods of field experimentation that have influenced the economics literature. The first might well be thought of as the dawn of “field” experimentation: the work of Neyman and Fisher, who laid the experimental foundation in the 1920s and 1930s by conceptualizing randomization as an instrument to achieve identification via experimentation with agricultural plots. The second, the large-scale social experiments conducted by government agencies in the mid-twentieth century, moved the exploration from plots of land to groups of individuals. More recently, the nature and range of field experiments has expanded, with a diverse set of controlled experiments being completed outside of the typical laboratory environment. With this growth, the number and types of questions that can be explored using field experiments has grown tremendously. After discussing these three distinct phases, we speculate on the future of field experimental methods, a future that we envision including a strong collaborative effort with outside parties, most importantly private entities.}, number={1}, journal={European Economic Review}, author={Levitt, Steven D. and List, John A.}, year={2009}, month={Jan}, pages={1–18}, language={en} }

 @article{ListSadoffWagner2011, title={So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design}, volume={14}, ISSN={1573-6938}, DOI={10.1007/s10683-011-9275-7}, abstractNote={Experimental economics represents a strong growth industry. In the past several decades the method has expanded beyond intellectual curiosity, now meriting consideration alongside the other more traditional empirical approaches used in economics. Accompanying this growth is an influx of new experimenters who are in need of straightforward direction to make their designs more powerful. This study provides several simple rules of thumb that researchers can apply to improve the efficiency of their experimental designs. We buttress these points by including empirical examples from the literature.}, number={4}, journal={Experimental Economics}, author={List, John A. and Sadoff, Sally and Wagner, Mathis}, year={2011}, month={Nov}, pages={439–457}, language={en} }

 @article{BloomRichburg-HayesBlack2007, title={Using Covariates to Improve Precision for Studies That Randomize Schools to Evaluate Educational Interventions}, volume={29}, ISSN={0162-3737}, DOI={10.3102/0162373707299550}, abstractNote={This article examines how controlling statistically for baseline covariates, especially pretests, improves the precision of studies that randomize schools to measure the impacts of educational interventions on student achievement. Empirical findings from five urban school districts indicate that (1) pretests can reduce the number of randomized schools needed for a given level of precision to about half of what would be needed otherwise for elementary schools, one fifth for middle schools, and one tenth for high schools, and (2) school-level pretests are as effective in this regard as student-level pretests. Furthermore, the precision-enhancing power of pretests (3) declines only slightly as the number of years between the pretest and posttests increases; (4) improves only slightly with pretests for more than 1 baseline year; and (5) is substantial, even when the pretest differs from the posttest. The article compares these findings with past research and presents an approach for quantifying their uncertainty.}, number={1}, journal={Educational Evaluation and Policy Analysis}, publisher={American Educational Research Association}, author={Bloom, Howard S. and Richburg-Hayes, Lashawn and Black, Alison Rebeck}, year={2007}, month={Mar}, pages={30–59}, language={en} }


 @article{WangOgburnRosenblum2019, title={Analysis of covariance in randomized trials: More precision and valid confidence intervals, without model assumptions}, volume={75}, ISSN={1541-0420}, DOI={10.1111/biom.13062}, abstractNote={“Covariate adjustment” in the randomized trial context refers to an estimator of the average treatment effect that adjusts for chance imbalances between study arms in baseline variables (called “covariates”). The baseline variables could include, for example, age, sex, disease severity, and biomarkers. According to two surveys of clinical trial reports, there is confusion about the statistical properties of covariate adjustment. We focus on the analysis of covariance (ANCOVA) estimator, which involves fitting a linear model for the outcome given the treatment arm and baseline variables, and trials that use simple randomization with equal probability of assignment to treatment and control. We prove the following new (to the best of our knowledge) robustness property of ANCOVA to arbitrary model misspecification: Not only is the ANCOVA point estimate consistent (as proved by Yang and Tsiatis, 2001) but so is its standard error. This implies that confidence intervals and hypothesis tests conducted as if the linear model were correct are still asymptotically valid even when the linear model is arbitrarily misspecified, for example, when the baseline variables are nonlinearly related to the outcome or there is treatment effect heterogeneity. We also give a simple, robust formula for the variance reduction (equivalently, sample size reduction) from using ANCOVA. By reanalyzing completed randomized trials for mild cognitive impairment, schizophrenia, and depression, we demonstrate how ANCOVA can achieve variance reductions of 4 to 32%.}, number={4}, journal={Biometrics}, author={Wang, Bingkai and Ogburn, Elizabeth L. and Rosenblum, Michael}, year={2019}, month={Dec}, pages={1391–1400}, language={eng} }


  @book{MurphyMyorsWolach2014, address={New York}, edition={4th edition}, title={Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests, Fourth Edition}, ISBN={978-1-84872-588-1}, abstractNote={Noted for its accessible approach, this text applies the latest approaches of power analysis to both null hypothesis and minimum-effect testing using the same basic unified model. Through the use of a few simple procedures and examples, the authors show readers with little expertise in statistical analysis how to obtain the values needed to carry out the power analysis for their research. Illustrations of how these analyses work and how they can be used to choose the appropriate criterion for defining statistically significant outcomes are sprinkled throughout. The book presents a simple and general model for statistical power analysis based on the F statistic and reviews how to determine: the sample size needed to achieve desired levels of power; the level of power needed in a study; the size of effect that can be reliably detected by a study; and sensible criteria for statistical significance. The book helps readers design studies, diagnose existing studies, and understand why hypothesis tests come out out the way they do.The fourth edition features:-New Boxed Material sections provide examples of power analysis in action and discuss unique issues that arise as a result of applying power analyses in different designs.-Many more worked examples help readers apply the concepts presented.-Expanded coverage of power analysis for multifactor analysis of variance (ANOVA) to show readers how to analyze up to four factors with repeated measures on any or all of the factors.-Re-designed and expanded web based One Stop F Calculator software and data sets that allow users to perform all of the book’s analyses and conduct significance tests, power analyses, and assessments of N and alpha needed for traditional and minimum-effects tests.-Easy to apply formulas for approximating the number of subjects required to reach adequate levels of power in a wide range of studies.Intended as a supplement for graduate/advanced undergraduate courses in research methods or experimental design, intermediate, advanced, or multivariate statistics, statistics II, or psychometrics, taught in psychology, education, business, and other social and health sciences, researchers also appreciate the book‘s applied approach.}, publisher={Routledge}, author={Murphy, Kevin R. and Myors, Brett and Wolach, Allen}, year={2014}, month={May}, language={English} }

   @article{KernanViscoliMakuchBrassHorwitz1999, title={Stratified randomization for clinical trials}, volume={52}, ISSN={0895-4356}, DOI={10.1016/s0895-4356(98)00138-3}, abstractNote={Trialists argue about the usefulness of stratified randomization. For investigators designing trials and readers who use them, the argument has created uncertainty regarding the importance of stratification. In this paper, we review stratified randomization to summarize its purpose, indications, accomplishments, and alternatives. In order to identify research papers, we performed a Medline search for 1966-1997. The search yielded 33 articles that included original research on stratification or included stratification as the major focus. Additional resources included textbooks. Stratified randomization prevents imbalance between treatment groups for known factors that influence prognosis or treatment responsiveness. As a result, stratification may prevent type I error and improve power for small trials (<400 patients), but only when the stratification factors have a large effect on prognosis. Stratification has an important effect on sample size for active control equivalence trials, but not for superiority trials. Theoretical benefits include facilitation of subgroup analysis and interim analysis. The maximum desirable number of strata is unknown, but experts argue for keeping it small. Stratified randomization is important only for small trials in which treatment outcome may be affected by known clinical factors that have a large effect on prognosis, large trials when interim analyses are planned with small numbers of patients, and trials designed to show the equivalence of two therapies. Once the decision to stratify is made, investigators need to chose factors carefully and account for them in the analysis.}, number={1}, journal={Journal of Clinical Epidemiology}, author={Kernan, W. N. and Viscoli, C. M. and Makuch, R. W. and Brass, L. M. and Horwitz, R. I.}, year={1999}, month={Jan}, pages={19–26}, language={eng} }


    @article{Vazquez-Bare2022, title={Identification and estimation of spillover effects in randomized experiments}, ISSN={0304-4076}, url={https://www.sciencedirect.com/science/article/pii/S0304407621003067}, DOI={10.1016/j.jeconom.2021.10.014}, abstractNote={I study identification, estimation and inference for spillover effects in experiments where units’ outcomes may depend on the treatment assignments of other units within a group. I show that the commonly-used reduced-form linear-in-means regression identifies a weighted sum of spillover effects with some negative weights, and that the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. I propose nonparametric estimators for average direct and spillover effects that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. These findings are illustrated using data from a conditional cash transfer program and with simulations. The empirical results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation: the estimated difference in means and the reduced-form linear-in-means coefficients are all close to zero and statistically insignificant, whereas the nonparametric estimators I propose reveal large, nonlinear and significant spillover effects.}, journal={Journal of Econometrics}, author={Vazquez-Bare, Gonzalo}, year={2022}, month={Jan}, language={en} }

 @article{Lehr1992, title={Sixteen S-squared over D-squared: A relation for crude sample size estimates}, volume={11}, ISSN={1097-0258}, DOI={10.1002/sim.4780110811}, abstractNote={I suggest for memorization an equation for calculating approximate sample size requirements intended only for a specific set of values (80 per cent power for a two-tailed alpha = 0.05 test) which seems to occur often in biopharmaceutical research. After presenting the formula in terms of variance estimate s2 and effect size d, I derive a few alternative forms and then discuss the accuracy of the approximation and other properties as well as examples of its use.}, number={8}, journal={Statistics in Medicine}, author={Lehr, Robert}, year={1992}, pages={1099–1102}, language={en} }


 @article{Schochet2013, title={Statistical Power for School-Based RCTs With Binary Outcomes}, volume={6}, ISSN={1934-5747}, DOI={10.1080/19345747.2012.725803}, abstractNote={This article develops a new approach for calculating appropriate sample sizes for school-based randomized control trials (RCTs) with binary outcomes using logit models with and without baseline covariates. The theoretical analysis develops sample size formulas for clustered designs where random assignment is at the school or teacher level using generalized estimating equation methods. The article focuses on the impact parameter pertaining to rates and proportions rather than to the log odds of response, which has been the focus of the previous literature. The article also compiles intraclass correlations (ICCs) for the clustered design for a range of binary outcomes using data from seven education RCTs. These ICCs and the power formulas are then used to conduct a power analysis using a provided SAS macro; the key finding is that sample sizes of 40 to 60 schools that are typically included in clustered RCTs for student test score or behavioral scale outcomes will often be insufficient for binary outcomes. A key reason is that the potential for precision gains from regression adjustment is likely to be smaller for binary outcomes.}, number={3}, journal={Journal of Research on Educational Effectiveness}, publisher={Routledge}, author={Schochet, Peter Z.}, year={2013}, month={Jul}, pages={263–294} }

